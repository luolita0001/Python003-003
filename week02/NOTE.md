###学习笔记

学习上遇到的问题：在利用scrapy框架写爬虫时，对于哪部分功能用scrapy的哪个组件书写比较模糊，对类的继承的基础不够牢固。

这周学习了一些反爬虫的技巧，和保存数据的方法。

反爬虫方面：

1.模拟浏览器的头部信息（headers），包括user-agent(referer)。可以利用fake_useragent获取虚拟的客户端信息；referer主要是（登陆/点击）网页进行跳转后需要写入跳转之前的网页（也可以用于获取二维码图片）

2.cookies验证，cookies上记载了用户的登录信息，使用requests.Session()方法可以在同一会话中使用相同的cookies,可以实现用户下次登陆的功能（cookies有有效期）

3.webdriver模拟浏览器行为，webdriver可以帮助我们获取动态网页的数据

4.验证码识别，利用第三方库pillow对图片做灰度化，二值化处理，使得验证码中的字符和背景区分开来

5.代理ip，用一个ip爬取网页时的危险度较高，这时可以使用代理ip,免费代理ip可以在github上获取代理池（但自己不会使用。。。）



保存数据的方式：

1.简单存储：利用pymsql库创建连接，创建游标，执行sql语句，关闭游标，关闭连接

2.爬取大量数据时可以使用分布式数据库redis

